{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8378d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31047cf",
   "metadata": {},
   "source": [
    "\\section{Data processing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b3d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing Smiles:  0\n",
      "Number of positive examples:  46\n"
     ]
    }
   ],
   "source": [
    "# Read data for positive examples of GABA receptor modulators\n",
    "\n",
    "folder_path = \"../data/raw/iuphar\"\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Load allosteric modulator data for each subunit into one df\n",
    "for file_name in os.listdir(folder_path):\n",
    "    tmp = pd.read_csv(f\"{folder_path}/{file_name}\")\n",
    "    tmp.insert(0, 'Target', file_name.replace('.csv', ''))\n",
    "    tmp['ChEMBL_ID'] = tmp['ChEMBL_ID'].str.strip()\n",
    "    data = pd.concat([data, tmp], ignore_index=True)\n",
    "\n",
    "file_path = \"../data/raw/ChEMBL-gaba;anion-bioactivities-991.csv\"\n",
    "all = pd.read_csv(file_path, delimiter=';')\n",
    "all.rename(columns={\"Molecule ChEMBL ID\": \"ChEMBL_ID\"}, inplace=True)\n",
    "\n",
    "# Populate Smiles strings based on ChEMBL_ID and Compound name\n",
    "merged_data = pd.merge(data, all[['ChEMBL_ID', 'Smiles']], on='ChEMBL_ID', how='left')\n",
    "data['Smiles'] = merged_data['Smiles']\n",
    "\n",
    "chembl_to_smiles = {\n",
    "    \"CHEMBL646\": \"Cc1nnc2n1-c1ccc(Cl)cc1C(c1ccccc1Cl)=NC2\",\n",
    "    \"CHEMBL407\": \"CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2\",\n",
    "    \"CHEMBL3681419\": \"Cc1onc(-c2ccc(F)cc2)c1COc1ccc(C(=O)N2CCS(=O)(=O)CC2)cn1\",\n",
    "    \"CHEMBL13280\": \"CN1C(=O)CN=C(c2ccccc2F)c2cc([N+](=O)[O-])ccc21\",\n",
    "    \"CHEMBL661\": \"Cc1nnc2n1-c1ccc(Cl)cc1C(c1ccccc1)=NC2\",\n",
    "    \"CHEMBL413325\": \"COC(=O)c1cc(-c2ccc(OC)cc2)c(-c2ccncc2)n(C)c1=O\",\n",
    "    \"CHEMBL306422\": \"Cc1cc(-c2nnc3c4ccccc4c(OCc4cn(C)nn4)nn23)no1\",\n",
    "    \"CHEMBL207538\":\"CC(=O)[C@H]1CC[C@H]2[C@@H]3CC[C@H]4C[C@H](O)CC[C@]4(C)[C@H]3CC[C@]12C\",\n",
    "    \"CHEMBL366947\":\"CC(C)(C)OC(=O)c1ncn2c1[C@@H]1CCCN1C(=O)c1c(Br)cccc1-2\",\n",
    "    \"CHEMBL373250\": \"Cn1ncnc1COc1nn2c(-c3cc(F)ccc3F)nnc2cc1C(C)(C)C\",\n",
    "    \"CHEMBL363211\": \"Cc1cc(-c2nncc3c(C(C)(C)C)c(OCc4ncnn4C)nn23)no1\",\n",
    "    \"CHEMBL2105199\": \"O=C(c1ccccn1)c1cnn2c(-c3ccncc3)ccnc12\",\n",
    "    \"CHEMBL6597\": \"CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(N=[N+]=[N-])ccc1-2\",\n",
    "    \"CHEMBL269366\": \"CN1Cc2c(C(=O)OC(C)(C)C)ncn2-c2ccsc2C1=O\",\n",
    "    \"CHEMBL1080588\": \"FC(F)c1ncn2c1Cn1ncnc1-c1cc(Br)ccc1-2\",\n",
    "    \"CHEMBL58757\": \"C#Cc1ccc2c(c1)C(=O)N(C)Cc1c(C(=O)OC(C)(C)C)ncn1-2\",\n",
    "    \"CHEMBL1256760\": \"C[C@]12CC[C@@H](O)C[C@@H]1CC[C@@H]1[C@@H]2CC[C@]2(C)[C@@H](C(=O)CO)CC[C@@H]12\",\n",
    "    \"CHEMBL1271047\": \"CCOC(=O)c1ncc2[nH]c3cccc(OC(C)C)c3c2c1C\",\n",
    "    \"CHEMBL3989949\": \"NC(=O)O[C@@H](Cn1ncnn1)c1ccccc1Cl\",\n",
    "    \"CHEMBL6597\": \"CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(N=[N+]=[N-])ccc1-2\",\n",
    "    \"CHEMBL945\": \"N=C(N)NC(=O)c1nc(Cl)c(N)nc1N\",\n",
    "    \"CHEMBL35\": \"NS(=O)(=O)c1cc(C(=O)O)c(NCc2ccco2)cc1Cl\",\n",
    "    \"CHEMBL4087508\": \"COc1cccc(-n2nc3c4cc(Cl)ccc4[nH]cc-3c2=O)c1\",\n",
    "    \"CHEMBL4092332\": \"COc1ccc(-n2nc3c4ccc(Br)cc4[nH]cc-3c2=O)cc1\",\n",
    "    \"CHEMBL1783282\": \"CCCNC(=O)c1nnc2c(-c3c(F)cccc3OC)cccc2c1N\",\n",
    "    \"CHEMBL452\": \"O=C1CN=C(c2ccccc2Cl)c2cc([N+](=O)[O-])ccc2N1\",\n",
    "    \"CHEMBL1568698\": \"CC(=O)[C@H]1CC[C@H]2[C@@H]3CC[C@H]4C[C@](C)(O)CC[C@]4(C)[C@H]3CC[C@]12C\",\n",
    "    \"CHEMBL262075\": \"CC(=O)N(C)c1cccc(-c2ccnc3c(C(=O)c4cccs4)cnn23)c1\",\n",
    "    \"CHEMBL373250\": \"Cn1ncnc1COc1nn2c(-c3cc(F)ccc3F)nnc2cc1C(C)(C)C\",\n",
    "    \"CHEMBL269366\": \"CN1Cc2c(C(=O)OC(C)(C)C)ncn2-c2ccsc2C1=O\",\n",
    "    \"CHEMBL911\": \"Cc1ccc(-c2nc3ccc(C)cn3c2CC(=O)N(C)C)cc1\",\n",
    "    \"CHEMBL200177\": \"CCn1ncnc1COc1nn2c(-c3ccccc3F)nnc2cc1C(C)(C)C\",\n",
    "   \"CHEMBL43948\": \"COC1=CC=C(C=C1)N2C(=O)C3=CN=C4C=C(C=CC4=C3N2)OC\" # From PubChem\n",
    "}\n",
    "compound_to_smiles = {\n",
    "    \"compound 20 [PMID: 35584373]\": \"CC1=C(C(=NO1)C2=CC=C(C=C2)F)COC3=NC4=C(CN(CC4)C(=O)CS(=O)(=O)C)C=C3\",\n",
    "    \"ONO-8590580\": \"CC1=C(C(=CC2=C1N=CN2CC3CC3)NC4=NC=C(C=C4)C5=CN(C=N5)C)F\",\n",
    "    \"Zn2+\" : \"[Zn+2]\",\n",
    "    \"TP003\": \"CC(C)(C1=C(C2=NC=C(N2C=C1)C3=CC(=C(C=C3)F)C4=C(C=C(C=C4)F)C#N)F)O\",\n",
    "    \"[3H]CGS8216\": \"C1=CC=C(C=C1)N2C(=O)C3=C(N2)C4=CC=CC=C4N=C3\",\n",
    "    \"[18F]fluoroethylflumazenil\": \"CCOC(=O)C1=C2CN(C(=O)C3=C(N2C=N1)C=CC(=C3)F)CC[18F]\",\n",
    "    \"DMCM\": \"CCC1=C2C3=CC(=C(C=C3NC2=CN=C1C(=O)OC)OC)OC\"\n",
    "}\n",
    "\n",
    "data['Smiles'] = data.apply(\n",
    "    lambda row: chembl_to_smiles.get(row['ChEMBL_ID'], row['Smiles']), axis=1\n",
    ")\n",
    "data['Smiles'] = data.apply(\n",
    "    lambda row: compound_to_smiles.get(row['Compound'], row['Smiles']), axis=1\n",
    ")\n",
    "print(\"Number of missing Smiles: \", len(data[(data['Smiles'].isna())]))\n",
    "\n",
    "# Add label (all these molecules interact with GABA receptor)\n",
    "data['Interaction'] = 1\n",
    "\n",
    "# Remove duplicates based on the 'Compound' column, keeping the first instance\n",
    "data_mol_only = data.drop_duplicates(subset='Compound', keep='first')\n",
    "data_mol_only = data_mol_only.drop(columns=['Target', 'Species', 'Activity', 'Value', 'Unit', 'ChEMBL_ID'])\n",
    "\n",
    "data_mol_only.reset_index(drop=True, inplace=True) # reset indices\n",
    "\n",
    "data_pos = data_mol_only\n",
    "num_pos = len(data_pos)\n",
    "print('Number of positive examples: ', num_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b6c8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative examples (set to = num_pos):  46\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative examples (modulators of other targets)\n",
    "\n",
    "data_neg = pd.read_csv(\"../data/processed/iuphar_notgaba.csv\")\n",
    "data_mol_only = data_neg.drop_duplicates(subset='Compound', keep='first')\n",
    "data_mol_only = data_mol_only.drop(columns=['Target', 'Species', 'Action', 'Value', 'Parameter', 'Type'])\n",
    "\n",
    "data_neg = data_mol_only.sample(n=num_pos, random_state=42)\n",
    "data_neg.reset_index(drop=True, inplace=True)\n",
    "data_neg['Interaction'] = 0\n",
    "\n",
    "print('Number of negative examples (set to = num_pos): ', len(data_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1a3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg], ignore_index=True)\n",
    "data\n",
    "\n",
    "# Check if there are duplicate compounds\n",
    "duplicates = data[data.duplicated(subset='Compound', keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate compounds found:\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6006ef7",
   "metadata": {},
   "source": [
    "\\subsection{Create dataset 1 (bulked by adding RDKit features and Morgan fingerprints)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RDKIT descriptors:  217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:01:42] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fingerprint features:  1024\n",
      "46\n",
      "46\n",
      "46\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "# This code computes the RDKIT features and Morgan fingerprints. \n",
    "# It is not needed for MolBERT or ChemBERTA, which directly take in the SMILES strings.\n",
    "\n",
    "import importlib\n",
    "import compute_features\n",
    "importlib.reload(compute_features)\n",
    "extractor = compute_features.FeatureExtractor(data)\n",
    "data_bulked = extractor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ca3c2",
   "metadata": {},
   "source": [
    "\\subsection{Create dataset 2 (Smiles converted to featurized 3d structures)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b288828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import deepchem as dc\n",
    "\n",
    "# Function to convert SMILES to 3D structure\n",
    "def smiles_to_3d(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol, randomSeed=42)\n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    return mol\n",
    "\n",
    "\n",
    "data['Mol'] = data['Smiles'].apply(smiles_to_3d)\n",
    "\n",
    "featurizer = dc.feat.CircularFingerprint()\n",
    "X = featurizer.featurize(data['Mol'])\n",
    "X = np.array(X)\n",
    "y = data['Interaction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792bc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.to_csv(\"../data/processed/iuphar_labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67aa075",
   "metadata": {},
   "source": [
    "\\subsection{Create train/test/val split (same molecules across all datasets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 64, Validation size: 9, Test size: 19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert train_ratio + val_ratio + test_ratio == 1\n",
    "\n",
    "# Split the data into train and temp (val + test)\n",
    "train_indices, temp_indices = train_test_split(data.index, train_size=train_ratio, random_state=42)\n",
    "\n",
    "# Split the temp data into val and test\n",
    "val_indices, test_indices = train_test_split(temp_indices, test_size=test_ratio/(test_ratio + val_ratio), random_state=42)\n",
    "\n",
    "# Create the splits for data\n",
    "data_train = data.iloc[train_indices]\n",
    "data_val = data.iloc[val_indices]\n",
    "data_test = data.iloc[test_indices]\n",
    "\n",
    "# Create the splits for 3d featurized data\n",
    "X_train = X[train_indices]\n",
    "X_val = X[val_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_val = y[val_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(f\"Train size: {len(data_train)}, Validation size: {len(data_val)}, Test size: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee7ad4",
   "metadata": {},
   "source": [
    "\\section{Finetune DeepChem's GraphConvModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "03c5a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'split' is deprecated.  Use 'splitter' instead.\n"
     ]
    }
   ],
   "source": [
    "chembl_tasks, datasets, transformers = dc.molnet.load_chembl(\n",
    "   shard_size=2000, featurizer=\"GraphConv\", set=\"5thresh\", split=\"random\")\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (92,), y.shape: (92,), w.shape: (92,), ids: [0 1 2 ... 89 90 91], task_names: [0]>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to 3d molecular rep \n",
    "featurizer = dc.feat.ConvMolFeaturizer(per_atom_fragmentation=False)\n",
    "f = featurizer.featurize(data['Smiles'].to_list())\n",
    "dataset = dc.data.NumpyDataset(X=f, y=np.array(data['Interaction']))\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eaab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 73, Validation size: 9, Test size: 10\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train, test, val\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset)\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(valid_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "68a09b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deepchem.models.torch_models' has no attribute 'GraphConvModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphConvModel\u001b[49m(n_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, nb_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deepchem.models.torch_models' has no attribute 'GraphConvModel'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = dc.models.torch_models.GraphConvModel(n_tasks=1, mode='classification')\n",
    "model.fit(train_dataset, nb_epoch=10)\n",
    "test_preds = model.predict(test_dataset)\n",
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113914f",
   "metadata": {},
   "source": [
    "\\section{Finetune MolBERT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad27895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a94638ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>Mol</th>\n",
       "      <th>Graph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triazolam</td>\n",
       "      <td>Cc1nnc2n1-c1ccc(Cl)cc1C(c1ccccc1Cl)=NC2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39f86d0&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([6.]), tensor([7.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>compound 20 [PMID: 35584373]</td>\n",
       "      <td>CC1=C(C(=NO1)C2=CC=C(C=C2)F)COC3=NC4=C(CN(CC4)...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39f8820&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([6.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flumazenil</td>\n",
       "      <td>CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39f8890&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([6.]), tensor([8.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basmisanil</td>\n",
       "      <td>Cc1onc(-c2ccc(F)cc2)c1COc1ccc(C(=O)N2CCS(=O)(=...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39f8270&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([6.]), tensor([8.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flunitrazepam</td>\n",
       "      <td>CN1C(=O)CN=C(c2ccccc2F)c2cc([N+](=O)[O-])ccc21</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39f8900&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([7.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ORG-37684</td>\n",
       "      <td>COc1ccc2c(c1OC1CNCC1)CCC2</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39fddd0&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([8.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>haloperidol</td>\n",
       "      <td>Fc1ccc(cc1)C(=O)CCCN1CCC(CC1)(O)c1ccc(cc1)Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39fde40&gt;</td>\n",
       "      <td>[(x, [tensor([9.]), tensor([6.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>fluspirilene</td>\n",
       "      <td>Fc1ccc(cc1)C(c1ccc(cc1)F)CCCN1CCC2(CC1)C(=O)NC...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39fdeb0&gt;</td>\n",
       "      <td>[(x, [tensor([9.]), tensor([6.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(+)-UH232</td>\n",
       "      <td>CCCN(C1C=Cc2c(C1C)cccc2OC)CCC</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39fdf20&gt;</td>\n",
       "      <td>[(x, [tensor([6.]), tensor([6.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NGB 2904</td>\n",
       "      <td>O=C(c1ccc2c(c1)Cc1c2cccc1)NCCCCN1CCN(CC1)c1ccc...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x1a39fdf90&gt;</td>\n",
       "      <td>[(x, [tensor([8.]), tensor([6.]), tensor([6.])...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Compound  \\\n",
       "0                      triazolam   \n",
       "1   compound 20 [PMID: 35584373]   \n",
       "2                     flumazenil   \n",
       "3                     basmisanil   \n",
       "4                  flunitrazepam   \n",
       "..                           ...   \n",
       "87                     ORG-37684   \n",
       "88                   haloperidol   \n",
       "89                  fluspirilene   \n",
       "90                     (+)-UH232   \n",
       "91                      NGB 2904   \n",
       "\n",
       "                                               Smiles  Interaction  \\\n",
       "0             Cc1nnc2n1-c1ccc(Cl)cc1C(c1ccccc1Cl)=NC2            1   \n",
       "1   CC1=C(C(=NO1)C2=CC=C(C=C2)F)COC3=NC4=C(CN(CC4)...            1   \n",
       "2             CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2            1   \n",
       "3   Cc1onc(-c2ccc(F)cc2)c1COc1ccc(C(=O)N2CCS(=O)(=...            1   \n",
       "4      CN1C(=O)CN=C(c2ccccc2F)c2cc([N+](=O)[O-])ccc21            1   \n",
       "..                                                ...          ...   \n",
       "87                          COc1ccc2c(c1OC1CNCC1)CCC2            0   \n",
       "88       Fc1ccc(cc1)C(=O)CCCN1CCC(CC1)(O)c1ccc(cc1)Cl            0   \n",
       "89  Fc1ccc(cc1)C(c1ccc(cc1)F)CCCN1CCC2(CC1)C(=O)NC...            0   \n",
       "90                      CCCN(C1C=Cc2c(C1C)cccc2OC)CCC            0   \n",
       "91  O=C(c1ccc2c(c1)Cc1c2cccc1)NCCCCN1CCN(CC1)c1ccc...            0   \n",
       "\n",
       "                                              Mol  \\\n",
       "0   <rdkit.Chem.rdchem.Mol object at 0x1a39f86d0>   \n",
       "1   <rdkit.Chem.rdchem.Mol object at 0x1a39f8820>   \n",
       "2   <rdkit.Chem.rdchem.Mol object at 0x1a39f8890>   \n",
       "3   <rdkit.Chem.rdchem.Mol object at 0x1a39f8270>   \n",
       "4   <rdkit.Chem.rdchem.Mol object at 0x1a39f8900>   \n",
       "..                                            ...   \n",
       "87  <rdkit.Chem.rdchem.Mol object at 0x1a39fddd0>   \n",
       "88  <rdkit.Chem.rdchem.Mol object at 0x1a39fde40>   \n",
       "89  <rdkit.Chem.rdchem.Mol object at 0x1a39fdeb0>   \n",
       "90  <rdkit.Chem.rdchem.Mol object at 0x1a39fdf20>   \n",
       "91  <rdkit.Chem.rdchem.Mol object at 0x1a39fdf90>   \n",
       "\n",
       "                                                Graph  \n",
       "0   [(x, [tensor([6.]), tensor([6.]), tensor([7.])...  \n",
       "1   [(x, [tensor([6.]), tensor([6.]), tensor([6.])...  \n",
       "2   [(x, [tensor([6.]), tensor([6.]), tensor([8.])...  \n",
       "3   [(x, [tensor([6.]), tensor([6.]), tensor([8.])...  \n",
       "4   [(x, [tensor([6.]), tensor([7.]), tensor([6.])...  \n",
       "..                                                ...  \n",
       "87  [(x, [tensor([6.]), tensor([8.]), tensor([6.])...  \n",
       "88  [(x, [tensor([9.]), tensor([6.]), tensor([6.])...  \n",
       "89  [(x, [tensor([9.]), tensor([6.]), tensor([6.])...  \n",
       "90  [(x, [tensor([6.]), tensor([6.]), tensor([6.])...  \n",
       "91  [(x, [tensor([8.]), tensor([6.]), tensor([6.])...  \n",
       "\n",
       "[92 rows x 5 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mol_to_graph(mol):\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(atom.GetAtomicNum())  # Use atomic number as feature\n",
    "\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        edge_index.append([start, end])\n",
    "        edge_index.append([end, start])  # Undirected graph\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)  # Node features\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Edge list\n",
    "\n",
    "    # Extract 3D coordinates\n",
    "    pos = torch.tensor(mol.GetConformer().GetPositions(), dtype=torch.float)\n",
    "\n",
    "    # Create PyTorch Geometric graph object\n",
    "    graph = Data(x=x, edge_index=edge_index, pos=pos)\n",
    "    return graph\n",
    "\n",
    "data['Graph'] = data['Mol'].apply(mol_to_graph)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch version: no pretrained checkpoint\n",
    "from torch_geometric.nn.models import DimeNet, DimeNetPlusPlus\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch.geometric.loader import DataLoader\n",
    "\n",
    "Model = DimeNetPlusPlus\n",
    "dataset = QM9()\n",
    "\n",
    "model, data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75152c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tensorflow version - pretrained checkpoint from GitHub repo\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNetPP\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f20e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_pp.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "model_name = config['model_name']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae0874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5f4a4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c49638f0",
   "metadata": {},
   "source": [
    "\\section{Misc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b24a90-5315-4161-aae2-d0a85614f20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684dd461a47e46b3b99fa0bbfdbd4b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/9.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1644702d185645e68e55cc73aa8f1a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lianwang/opt/anaconda3/envs/cs229-final-project/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1905: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0894846b3164e8998e71721bac756ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=767, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9dea3aa4a24019b54076bc90005535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f61269c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Smiles  pChEMBL Value  \\\n",
      "0               CCCCCCCC/C=C\\CCCCCCCC(=O)NC(CO)(CO)CO       5.443697   \n",
      "1     CCOC(=O)c1ncn2c1[C@@H]1CCCN1C(=O)c1cc(OC)ccc1-2       7.770000   \n",
      "2              CC(C)(C)OC(=O)c1cc2c(cn1)[nH]c1ccccc12       7.820000   \n",
      "3          CC(OC(=O)c1cc2c(cn1)[nH]c1ccccc12)c1ccccc1       6.000000   \n",
      "4          CC(OC(=O)c1cc2c(cn1)[nH]c1ccccc12)c1ccccc1       6.000000   \n",
      "5              c1ccc2c(c1)[nH]c1cnc3c4ccncc4[nH]c3c12       7.620000   \n",
      "6        Cc1cc(-c2nnc3c4ccccc4c(OCc4cn(C)nn4)nn23)no1       9.240000   \n",
      "7     Cc1c(F)c(Nc2ccc(-c3cn(C)cn3)cn2)cc2c1ncn2CC1CC1       7.500000   \n",
      "8   C#Cc1ccc2c(c1)C(c1ccccc1F)=N[C@H](C)c1c(C(=O)N...       6.440000   \n",
      "9           C#Cc1ccc2[nH]c3cnc(C(=O)OC(C)(C)C)cc3c2c1       6.960000   \n",
      "10             c1ccc2c(c1)[nH]c1cnc3c4cccnc4[nH]c3c12       7.920000   \n",
      "11          CCCNC(=O)c1nnc2c(-c3cc(OC)ccc3OC)cccc2c1N       8.300000   \n",
      "12            CCCNC(=O)c1nnc2c(-c3cc(C)ccc3C)cccc2c1N       8.260000   \n",
      "13           Clc1ccc2c(c1)[nH]c1c2ncc2[nH]c3ccccc3c21       7.920000   \n",
      "14             c1ccc2c(c1)[nH]c1cnc3c4cccnc4[nH]c3c12       7.920000   \n",
      "15                       CCCOc1cc2c(cn1)[nH]c1ccccc12       7.280000   \n",
      "16           COc1ccc2c(c1)[nH]c1c2ncc2[nH]c3ccccc3c21       7.920000   \n",
      "17               FC(F)c1ncn2c1Cn1ncnc1-c1cc(Br)ccc1-2       6.730000   \n",
      "18  C#Cc1ccc2c(c1)C(c1ccccc1F)=N[C@H](C)c1c(C(=O)N...       6.100000   \n",
      "19           CCCNC(=O)c1nnc2c(-c3cc(C)ccc3OC)cccc2c1N       8.210000   \n",
      "20      CCOC(=O)c1ncc2[nH]c3ccc(OCc4ccccc4)cc3c2c1COC       8.400000   \n",
      "21        CCOC(=O)c1nc(C)c2[nH]c3cc(OC)c(OC)cc3c2c1CC       7.720000   \n",
      "22   Cc1csc(-c2cc(-c3ccccc3)c3n(c2=O)CCCc2ccncc2-3)n1       9.520000   \n",
      "23   Cc1csc(-c2cc(-c3ccncc3)c3n(c2=O)CCCc2ccncc2-3)n1       9.220000   \n",
      "24  COCc1c(C(=O)OC(C)C)ncc2[nH]c3ccc(OCc4ccccc4)cc...       8.700000   \n",
      "25       CCc1c(C(=O)OC)nc(C)c2[nH]c3cc(OC)c(OC)cc3c12       8.100000   \n",
      "26             c1ccc2c(c1)[nH]c1cnc3c4ccncc4[nH]c3c12       7.620000   \n",
      "27  C#Cc1ccc2c(c1)C(c1ccccc1F)=N[C@H](C)c1c(C(=O)N...       4.520000   \n",
      "28                Clc1ccc2c(c1)-c1ncnn1Cc1c(Cl)ncn1-2       7.620000   \n",
      "29           CCNC(=O)c1nnc2c(-c3cc(OC)ccc3OC)cccc2c1N       7.530000   \n",
      "30    COc1ccc(OC)c(-c2cccc3c(N)c(C(=O)NC(C)C)nnc23)c1       7.250000   \n",
      "31    C#Cc1ccc2c(c1)C(c1ccccn1)=NCc1c(-c3cnco3)ncn1-2       6.930000   \n",
      "32                   CCCC(=O)c1cc2c(cn1)[nH]c1ccccc12       7.800000   \n",
      "33              CCCNC(=O)c1nnc2c(-c3cnccc3OC)cccc2c1N       8.650000   \n",
      "34                COCc1ncn2c1CN(C)C(=O)c1cc(Cl)ccc1-2       6.750000   \n",
      "35     C#Cc1ccc2c(c1)C(c1ccccn1)=NCc1c(C(=O)OC)ncn1-2       6.000000   \n",
      "36          CCCNC(=O)c1nnc2c(-c3cc(OC)ccc3OC)cccc2c1N       7.820000   \n",
      "37             c1ccc2c(c1)[nH]c1c2ncc2[nH]c3ccccc3c21       9.000000   \n",
      "38          CCCNC(=O)c1nnc2c(-c3cnc(OC)nc3OC)cccc2c1N       8.130000   \n",
      "39    COc1ccc(OC)c(-c2cccc3c(N)c(C(=O)NC4CC4)nnc23)c1       8.000000   \n",
      "40   COc1ccc(OC)c(-c2cccc3c(N)c(C(=O)NC4CCC4)nnc23)c1       7.580000   \n",
      "41     Cc1cc(-c2nncc3c(C(C)(C)C)c(OCc4ncnn4C)nn23)no1       9.070000   \n",
      "42     Cn1ncnc1COc1nn2c(-c3cc(F)ccc3F)nnc2cc1C(C)(C)C       9.150000   \n",
      "43                   CCOC(=O)c1cc2c(cn1)[nH]c1ccccc12       8.300000   \n",
      "44        COC(=O)c1cc2c(cn1)[nH]c1ccc(NCc3ccccc3)cc12       7.520000   \n",
      "45               CCOC(=O)c1ncc2[nH]c3ccc(O)cc3c2c1COC       8.300000   \n",
      "46              CCOC(=O)c1ncc2[nH]c3ccc(OC)cc3c2c1COC       9.000000   \n",
      "47          CCCOc1ccc2[nH]c3cnc(C(=O)OCC)c(COC)c3c2c1       9.000000   \n",
      "48    CCOC(=O)c1ncn2c1[C@@H]1CCCN1C(=O)c1cc(OC)ccc1-2       7.570000   \n",
      "49     Cc1cc(-c2nncc3c(C(C)(C)C)c(OCc4ncnn4C)nn23)no1       9.100000   \n",
      "50           CCCNC(=O)c1nnc2c(-c3c(F)cccc3OC)cccc2c1N       9.510000   \n",
      "51                  CCOC(=O)c1ncc2[nH]c3ccccc3c2c1COC       8.520000   \n",
      "52  CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(N=[N+]=[N-])ccc1-2       8.150000   \n",
      "53                    COC(=O)c1cc2c(cn1)[nH]c1ccccc12       8.150000   \n",
      "54                CN1C(=O)CN=C(c2ccccc2)c2cc(Cl)ccc21       7.770000   \n",
      "55            CCOC(=O)c1ncn2c1CN(C)C(=O)c1cc(F)ccc1-2       9.100000   \n",
      "56  Cc1onc(-c2ccc(F)cc2)c1COc1ccc(C(=O)N2CCS(=O)(=...       6.300000   \n",
      "57   c1ccc(-c2nnc3c4c(c(OCc5ccccn5)nn23)C2CCC4CC2)cc1       7.580000   \n",
      "58           CCCNC(=O)c1nnc2c(-c3c(F)cccc3OC)cccc2c1N       9.520000   \n",
      "59  CC(C)(O)c1cnn2c(-c3ccc(F)c(-c4c(F)cccc4C#N)c3)...       8.700000   \n",
      "\n",
      "                         tokens  \n",
      "0   [input_ids, attention_mask]  \n",
      "1   [input_ids, attention_mask]  \n",
      "2   [input_ids, attention_mask]  \n",
      "3   [input_ids, attention_mask]  \n",
      "4   [input_ids, attention_mask]  \n",
      "5   [input_ids, attention_mask]  \n",
      "6   [input_ids, attention_mask]  \n",
      "7   [input_ids, attention_mask]  \n",
      "8   [input_ids, attention_mask]  \n",
      "9   [input_ids, attention_mask]  \n",
      "10  [input_ids, attention_mask]  \n",
      "11  [input_ids, attention_mask]  \n",
      "12  [input_ids, attention_mask]  \n",
      "13  [input_ids, attention_mask]  \n",
      "14  [input_ids, attention_mask]  \n",
      "15  [input_ids, attention_mask]  \n",
      "16  [input_ids, attention_mask]  \n",
      "17  [input_ids, attention_mask]  \n",
      "18  [input_ids, attention_mask]  \n",
      "19  [input_ids, attention_mask]  \n",
      "20  [input_ids, attention_mask]  \n",
      "21  [input_ids, attention_mask]  \n",
      "22  [input_ids, attention_mask]  \n",
      "23  [input_ids, attention_mask]  \n",
      "24  [input_ids, attention_mask]  \n",
      "25  [input_ids, attention_mask]  \n",
      "26  [input_ids, attention_mask]  \n",
      "27  [input_ids, attention_mask]  \n",
      "28  [input_ids, attention_mask]  \n",
      "29  [input_ids, attention_mask]  \n",
      "30  [input_ids, attention_mask]  \n",
      "31  [input_ids, attention_mask]  \n",
      "32  [input_ids, attention_mask]  \n",
      "33  [input_ids, attention_mask]  \n",
      "34  [input_ids, attention_mask]  \n",
      "35  [input_ids, attention_mask]  \n",
      "36  [input_ids, attention_mask]  \n",
      "37  [input_ids, attention_mask]  \n",
      "38  [input_ids, attention_mask]  \n",
      "39  [input_ids, attention_mask]  \n",
      "40  [input_ids, attention_mask]  \n",
      "41  [input_ids, attention_mask]  \n",
      "42  [input_ids, attention_mask]  \n",
      "43  [input_ids, attention_mask]  \n",
      "44  [input_ids, attention_mask]  \n",
      "45  [input_ids, attention_mask]  \n",
      "46  [input_ids, attention_mask]  \n",
      "47  [input_ids, attention_mask]  \n",
      "48  [input_ids, attention_mask]  \n",
      "49  [input_ids, attention_mask]  \n",
      "50  [input_ids, attention_mask]  \n",
      "51  [input_ids, attention_mask]  \n",
      "52  [input_ids, attention_mask]  \n",
      "53  [input_ids, attention_mask]  \n",
      "54  [input_ids, attention_mask]  \n",
      "55  [input_ids, attention_mask]  \n",
      "56  [input_ids, attention_mask]  \n",
      "57  [input_ids, attention_mask]  \n",
      "58  [input_ids, attention_mask]  \n",
      "59  [input_ids, attention_mask]  \n"
     ]
    }
   ],
   "source": [
    "# Import dataset with SMILES and pChEMBL values\n",
    "data = pd.read_csv(\"../data/processed/ChEMBL-alpha2-bioactivities-274-bulked.csv\")\n",
    "data = data.dropna(subset=['Smiles', 'pChEMBL Value'])\n",
    "data = data[['Smiles', 'pChEMBL Value']]\n",
    "        \n",
    "nan_rows = data[data.isna().any(axis=1)]\n",
    "if not nan_rows.empty:\n",
    "    print(\"Rows with NaN values:\")\n",
    "    print(nan_rows)\n",
    "\n",
    "# Tokenize dataset\n",
    "data[\"tokens\"] = data[\"Smiles\"].apply(lambda x: tokenizer(x, padding='max_length', truncation=True, return_tensors='pt'))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MolBERTRegressor(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(MolBERTRegressor, self).__init__()\n",
    "        self.bert = base_model  # Pretrained MolBERT backbone\n",
    "        self.regressor = nn.Linear(768, 1)  # Output binding affinity score\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [batch_size, 768]\n",
    "        return self.regressor(pooled_output)  # Predict affinity\n",
    "\n",
    "# Initialize model\n",
    "fine_tune_model = MolBERTRegressor(model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce0998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68efb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(fine_tune_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Prepare data for training\n",
    "X_train = torch.stack([x[\"input_ids\"].squeeze(0) for x in data[\"tokens\"]]).to(device)\n",
    "attention_mask = torch.stack([x[\"attention_mask\"].squeeze(0) for x in data[\"tokens\"]]).to(device)\n",
    "y_train = torch.tensor(data[\"pChEMBL Value\"].values, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1157294c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'pooler_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs229-final-project/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs229-final-project/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36mMolBERTRegressor.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m     10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m---> 11\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooler_output\u001b[49m  \u001b[38;5;66;03m# [batch_size, 768]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(pooled_output)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'pooler_output'"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    fine_tune_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = fine_tune_model(X_train, attention_mask).squeeze()\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs229-final-project)",
   "language": "python",
   "name": "cs229-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
